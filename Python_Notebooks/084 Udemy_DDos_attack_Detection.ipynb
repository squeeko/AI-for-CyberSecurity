{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Udemy DDos attack Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8y6sXEvxeQh"
      },
      "source": [
        "DDos or Distributed Denial of Service is an attack in which traffic from different sources floods a victim resulting in service interruption.\n",
        "\n",
        "\n",
        "There are 3 types of DDos attacks: application-level, protocol and volumetric attacks.\n",
        "\n",
        "The dataset we will be working with is a subsampling of the CSE-CIC-IDS2018, CICIDS2017 and CICDos datasets(2017)\n",
        "\n",
        "It consists of 80% benign and 20% of DDos traffic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC9crbCuwyHZ",
        "outputId": "86296a5c-39d5-4892-9a3f-1a453d2514b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u5s4nqqw_rO",
        "outputId": "cc4cd748-8404-4e0f-808d-ebeb7400e7a1"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU4U3pl0yVSE"
      },
      "source": [
        "# import pandas and specify the data types for the columns.\n",
        "import pandas as pd\n",
        "features = [\n",
        "    \"Fwd Seg Size Min\",\n",
        "    \"Init Bwd Win Byts\",\n",
        "    \"Init Fwd Win Byts\",\n",
        "    \"Fwd Seg Size Min\",\n",
        "    \"Fwd Pkt Len Mean\",\n",
        "    \"Fwd Seg Size Avg\",\n",
        "    \"Label\",\n",
        "    \"Timestamp\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCy9Tdh9y30I"
      },
      "source": [
        "dtypes = {\n",
        "    \"Fwd Pkt Len Mean\": \"float\",\n",
        "    \"Fwd Seg Size Avg\": \"float\",\n",
        "    \"Init Fwd Win Byts\": \"int\",\n",
        "    \"Init Bwd Win Byts\": \"int\",\n",
        "    \"Fwd Seg Size Min\": \"int\",\n",
        "    \"Label\": \"str\",\n",
        "}\n",
        "date_columns = [\"Timestamp\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hRYqYlOzc5q"
      },
      "source": [
        "# Read in the .csv file containing the dataset\n",
        "df = pd.read_csv(\"ddos_dataset.csv\", usecols=features, dtype=dtypes,parse_dates=date_columns,index_col=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZIsOp9xzvTL"
      },
      "source": [
        "# Sort the data by date\n",
        "df2 = df.sort_values(\"Timestamp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WwEHHtez3wW"
      },
      "source": [
        "# We need to drop the date column as it is no longer needed\n",
        "df3 = df2.drop(columns=[\"Timestamp\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXjC2GeC0CVl"
      },
      "source": [
        "# We need to split the data into training and testing subsets, consisting of the first 80% and last 20% of the data\n",
        "l = len(df3.index)\n",
        "train_df = df3.head(int(l * 0.8))\n",
        "test_df = df3.tail(int(l * 0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD3w4Rbc0VxU"
      },
      "source": [
        "# Prepare the labels\n",
        "y_train = train_df.pop(\"Label\").values\n",
        "y_test = test_df.pop(\"Label\").values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2-Cpvv0hgv"
      },
      "source": [
        "# Prepare the feature vectors\n",
        "X_train = train_df.values\n",
        "X_test = test_df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvA5UY5F0qvR",
        "outputId": "b25ffa5d-0110-4ca8-92b9-4bf2d309bd65"
      },
      "source": [
        "# We need to import and instantiate a random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2woW30D2LC_"
      },
      "source": [
        "\n",
        "\n",
        "1.   Step 1: specifying a subset of features form our dataset, the ones we consider most promising as well as recoding their data type so we do not need to convert them later.\n",
        "\n",
        "1.   Step 2: We read the dataset into a data frame\n",
        "\n",
        "1.   Step 3 and 4: We sort the data by date since the problem requires being able to predict events in teh future and then drop the date column since we will not being employ it. \n",
        "\n",
        "1.   The next 2 steps we perform a train-test split.\n",
        "2.   Step 8 and 9: Depending on  the application the accuracy achieved is a good starting point. A promising direction to improve performance is to account for the source and destination IPs.\n"
      ]
    }
  ]
}