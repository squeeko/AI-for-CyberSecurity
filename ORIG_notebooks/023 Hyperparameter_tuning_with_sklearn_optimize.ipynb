{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter tuning with sklearn-optimize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3EvKQtOgzlQ"
      },
      "source": [
        "Hyperparmeter is a parameter whose value is set before the training process begins.\n",
        "\n",
        "\n",
        "The choice of a radient boosting model and the size of the hidden layer of a multilayer perceptron are examples of hyperparameters.\n",
        "\n",
        "Hyperparameter selection is important because it can have a huge effect ont he model's performance.\n",
        "\n",
        "The most basic approach for hyperparameter tuning is a grid search.\n",
        "\n",
        "In this method you specify a range of potential values for each hyperparameter, and then try them all out until you find the best combination.\n",
        "\n",
        "In this video you will learn how to use Bayesian optimization over hyperparameters using scikit-optimize.\n",
        "\n",
        "In Bayesian optimization, not all parameter values are tried out, but rather a fized number of parameter settings is sampleed from a specified distributions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbSR6e_hgkgY"
      },
      "source": [
        "#Load the wine dataset from scikit-learn\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "wine_dataset = datasets.load_wine()\n",
        "X = wine_dataset.data\n",
        "y = wine_dataset.target"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2DfSQQViYjd"
      },
      "source": [
        "# We need to import XGBoost and stratified K-fold:\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2n-D6p6jW2n",
        "outputId": "8bbc9816-a536-4b47-88ed-f476c18420af"
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-B3O99DilWC"
      },
      "source": [
        "# import BayesSerachCV from scikit-optimize and specify the number of parameter settings to test\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "n_iterations = 9"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9FOGwdzjdB1"
      },
      "source": [
        "#specify the estimator. In this case we select XGBoost and set it to be able to perform multi-class classification\n",
        "estimator = xgb.XGBClassifier(\n",
        "    n_jobs=-1,\n",
        "    objective=\"multi:softmax\",\n",
        "    eval_metric=\"merror\",\n",
        "    verbosity=0,\n",
        "    num_class=len(set(y)),\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFjLYLQdj5uN"
      },
      "source": [
        "# We need to specify a parameter search space\n",
        "search_space = {\n",
        "    \"learning_rate\": (0.01, 1.0, \"log-uniform\"),\n",
        "    \"min_child_weight\": (0, 10),\n",
        "    \"max_depth\": (1, 50),\n",
        "    \"max_delta_step\": (0, 10),\n",
        "    \"subsample\": (0.01, 1.0, \"uniform\"),\n",
        "    \"colsample_bytree\": (0.01, 1.0, \"log-uniform\"),\n",
        "    \"colsample_bylevel\": (0.01, 1.0, \"log-uniform\"),\n",
        "    \"reg_lambda\": (1e-9, 1000, \"log-uniform\"),\n",
        "    \"reg_alpha\": (1e-9, 1.0, \"log-uniform\"),\n",
        "    \"gamma\": (1e-9, 0.5, \"log-uniform\"),\n",
        "    \"min_child_weight\": (0, 5),\n",
        "    \"n_estimators\": (5, 5000),\n",
        "    \"scale_pos_weight\": (1e-6, 500, \"log-uniform\"),\n",
        "}"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5EC9hP0lDx8"
      },
      "source": [
        "# We need to specify the type of cross validation to perform\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zg9OSUNlQqT"
      },
      "source": [
        "# Define BayesSearchCV using the setting we have defined\n",
        "bayes_cv_tuner = BayesSearchCV(\n",
        "    estimator=estimator,\n",
        "    search_spaces=search_space,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    n_iter=n_iterations,\n",
        "    verbose=0,\n",
        "    refit=True,\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIGOucC8mYRb"
      },
      "source": [
        "# We need to define a callback function to printout the progress of the parameter search.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def print_status(optimal_result):\n",
        "    \"\"\"Shows the best parameters found and accuracy attained of the search so far.\"\"\"\n",
        "    models_tested = pd.DataFrame(bayes_cv_tuner.cv_results_)\n",
        "    best_parameters_so_far = pd.Series(bayes_cv_tuner.best_params_)\n",
        "    print(\n",
        "        \"Model #{}\\nBest accuracy so far: {}\\nBest parameters so far: {}\\n\".format(\n",
        "            len(models_tested),\n",
        "            np.round(bayes_cv_tuner.best_score_, 3),\n",
        "            bayes_cv_tuner.best_params_,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    clf_type = bayes_cv_tuner.estimator.__class__.__name__\n",
        "    models_tested.to_csv(clf_type + \"_cv_results_summary.csv\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6urwqMeIn92O",
        "outputId": "f43af448-8595-492e-d4ca-ee287012a7f5"
      },
      "source": [
        "# Perform the parameter search\n",
        "result = bayes_cv_tuner.fit(X, y, callback=print_status)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model #1\n",
            "Best accuracy so far: 0.966\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.022876613099071023), ('colsample_bytree', 0.3215617359264441), ('gamma', 5.4897801799682785e-08), ('learning_rate', 0.0282669721026536), ('max_delta_step', 6), ('max_depth', 7), ('min_child_weight', 4), ('n_estimators', 1704), ('reg_alpha', 0.3643003940715705), ('reg_lambda', 959), ('scale_pos_weight', 202), ('subsample', 0.5244594465916421)])\n",
            "\n",
            "Model #2\n",
            "Best accuracy so far: 0.978\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.176971047731701), ('colsample_bytree', 0.07429198119583591), ('gamma', 0.03255172716450493), ('learning_rate', 0.463579754987766), ('max_delta_step', 4), ('max_depth', 38), ('min_child_weight', 3), ('n_estimators', 4066), ('reg_alpha', 7.544523937853344e-07), ('reg_lambda', 46), ('scale_pos_weight', 225), ('subsample', 0.6843163117834636)])\n",
            "\n",
            "Model #3\n",
            "Best accuracy so far: 0.978\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.176971047731701), ('colsample_bytree', 0.07429198119583591), ('gamma', 0.03255172716450493), ('learning_rate', 0.463579754987766), ('max_delta_step', 4), ('max_depth', 38), ('min_child_weight', 3), ('n_estimators', 4066), ('reg_alpha', 7.544523937853344e-07), ('reg_lambda', 46), ('scale_pos_weight', 225), ('subsample', 0.6843163117834636)])\n",
            "\n",
            "Model #4\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n",
            "Model #5\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n",
            "Model #6\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n",
            "Model #7\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n",
            "Model #8\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n",
            "Model #9\n",
            "Best accuracy so far: 0.983\n",
            "Best parameters so far: OrderedDict([('colsample_bylevel', 0.1886395630639579), ('colsample_bytree', 0.6239237153528416), ('gamma', 0.002156807555741482), ('learning_rate', 0.6650014397093467), ('max_delta_step', 7), ('max_depth', 45), ('min_child_weight', 0), ('n_estimators', 3349), ('reg_alpha', 2.7711102714821863e-08), ('reg_lambda', 856), ('scale_pos_weight', 169), ('subsample', 0.15174902562345507)])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIzX7aOhpuw4"
      },
      "source": [
        "\n",
        "\n",
        "*   Step 1 and 2 we did import a standard dataset, the wine dataset as well as the libraries needed for classification. \n",
        "\n",
        "*   Step 3: we did specify how long we would like the search to be. **The longer the search, the better the results**\n",
        "*   Step 4: we did select XGBoost as the model and specify the number of classes, the type of problem and the evaluation metric.\n",
        "\n",
        "\n",
        "*   Step 5: we did specify a probability distribution over each parameter that we will be xploring. \n",
        "*   Step 6: we specify our corss-validation scheme. We did use stratified fold and for a regression problem stratified fold will be replaced with kFold.\n",
        "\n",
        "\n",
        "*   Step 7: we can see additional setting that can be changed. n_jobs allows you to parallelize the task\n",
        "*   Step 8: we did define a callback function to print out the progress.\n",
        "\n",
        "\n",
        "*   Step 9: We did run the hyper parameter search.\n"
      ]
    }
  ]
}