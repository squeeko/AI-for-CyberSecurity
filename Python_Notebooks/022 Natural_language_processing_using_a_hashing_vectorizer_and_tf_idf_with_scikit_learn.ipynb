{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural language processing using a hashing vectorizer and tf-idf with scikit-learn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L52mRMNlWoZk",
        "outputId": "5855dcf6-98e0-4c40-f48e-ff1779391b21"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhJnzpc-Y2Wa",
        "outputId": "4a64606b-af09-4df8-e680-5ec05ee2d076"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOAS9MtqbK1U"
      },
      "source": [
        "# First we need to import textual dataset\n",
        "with open(\"anonops_short.txt\", encoding=\"utf8\") as f:\n",
        "  anonops_chat_logs = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aao3wm8ybmhB"
      },
      "source": [
        "# We will count the words in the text using the hash vectorizer and then perform weighting using tf-idf\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB9eV9tMb6_A"
      },
      "source": [
        "my_vector = HashingVectorizer(input = \"content\", ngram_range=(1,2))\n",
        "x_train_counts = my_vector.fit_transform(anonops_chat_logs,)\n",
        "tf_transformer = TfidfTransformer(use_idf=True,).fit(x_train_counts)\n",
        "x_train_tf = tf_transformer.transform(x_train_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRpLLGUXcdmD",
        "outputId": "b392bd95-6d7a-4999-ed31-4ef8780c6b8e"
      },
      "source": [
        "# The end result is a sparse matrix with each row being a vector representing one of the texts\n",
        "x_train_tf\n",
        "print(x_train_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 938273)\t0.10023429482560929\n",
            "  (0, 871172)\t-0.33044470291777067\n",
            "  (0, 755834)\t-0.2806123960092745\n",
            "  (0, 556974)\t-0.2171490773135763\n",
            "  (0, 548264)\t-0.09851435603064428\n",
            "  (0, 531189)\t-0.2566310842337745\n",
            "  (0, 522961)\t-0.3119912982467716\n",
            "  (0, 514190)\t-0.2527659565181208\n",
            "  (0, 501800)\t-0.33044470291777067\n",
            "  (0, 499727)\t-0.18952297847436425\n",
            "  (0, 488876)\t0.13502094828386488\n",
            "  (0, 377854)\t0.22710724511856722\n",
            "  (0, 334594)\t-0.25581186158424035\n",
            "  (0, 256577)\t0.20949022238574433\n",
            "  (0, 197273)\t-0.30119674850360456\n",
            "  (0, 114899)\t0.09713499033205285\n",
            "  (0, 28523)\t-0.3060506288368513\n",
            "  (1, 960098)\t0.09780838928665199\n",
            "  (1, 955748)\t-0.2747271490090429\n",
            "  (1, 952302)\t0.26070217969901804\n",
            "  (1, 938273)\t0.12095603891963835\n",
            "  (1, 937092)\t-0.2947114257264502\n",
            "  (1, 927866)\t0.21727726371674563\n",
            "  (1, 820768)\t-0.11065660403137358\n",
            "  (1, 772066)\t-0.14344517367198276\n",
            "  :\t:\n",
            "  (180828, 329790)\t0.06808618130417012\n",
            "  (180828, 312887)\t-0.08249409552977467\n",
            "  (180828, 209871)\t0.17685927011939476\n",
            "  (180828, 193711)\t-0.14127016157231428\n",
            "  (180828, 181881)\t-0.11885031537539834\n",
            "  (180828, 180525)\t-0.06925490785130799\n",
            "  (180828, 156500)\t-0.20787461071537122\n",
            "  (180828, 148568)\t0.1963433059906426\n",
            "  (180828, 82508)\t-0.1289257787752738\n",
            "  (180828, 79994)\t0.23121076025389292\n",
            "  (180828, 78098)\t-0.18205107240120946\n",
            "  (180828, 47738)\t0.23121076025389292\n",
            "  (180828, 46353)\t0.1045181919567425\n",
            "  (180828, 45900)\t-0.09537730182105167\n",
            "  (180828, 45419)\t-0.11189579574426382\n",
            "  (180828, 11712)\t-0.16947494737589616\n",
            "  (180829, 1026910)\t0.4082112914772047\n",
            "  (180829, 975831)\t-0.18401193506169794\n",
            "  (180829, 936283)\t0.2472007199039777\n",
            "  (180829, 856299)\t-0.15436175878438183\n",
            "  (180829, 473183)\t-0.41092004816695277\n",
            "  (180829, 464504)\t0.2928849862993687\n",
            "  (180829, 251872)\t-0.4714000763194845\n",
            "  (180829, 189128)\t0.44418614795477124\n",
            "  (180829, 45900)\t-0.20102520636796686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jPABSaTcwOe"
      },
      "source": [
        "\n",
        "\n",
        "1.   Step 1: load the dataset. The anonops IRC channel has been affiliated with the anonymous hacktivist group. in particular, chat participants have in the past panned and announced their future targets on Anonops. Consequently, a well-engineered ML system will be able to predict cyber attacks on trainging on such data.\n",
        "\n",
        "1.   Step 2: We instantiated a hashing vectorier. The hasing vectorizer gave is counts of the 1- and 2-grams int eh text. In other words, singleton and consecutive pars of words (tokens). We then applied a tf-idf transformer to give appropriate wieghts to the counts that the hashing vectorizer gave us. So our final result is a large, sparse matrix representing the occurences of 1- and 2-grams int eh texts weighted importance.\n",
        "\n",
        "2.   Step 3: we did examine the frontend of a spars matrix representation of our featured data in Scipy.\n",
        "\n",
        "\n"
      ]
    }
  ]
}