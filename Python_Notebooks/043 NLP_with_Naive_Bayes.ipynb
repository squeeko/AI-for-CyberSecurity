{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP with Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWbCxREZ5_OG",
        "outputId": "eca69387-eb30-42d4-a5ac-03803f992829"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0SyVEMm6eIU",
        "outputId": "8cb7a28c-66ec-4b34-8604-5491e8ace761"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuoj7m2L7J1O"
      },
      "source": [
        "The phases that chracterzie NPL are\n",
        "\n",
        "1.   Identification of the words (tokens) constituting the language\n",
        "\n",
        "1.   Analysis of the strcuture of the text\n",
        "2.   Identification of the relationships between words(in paragrahs, sentences and so on)\n",
        "2.   Semantic analysis of the text\n",
        "\n",
        "\n",
        "The best known python libraries for NLP is Natural Language Toolkit (NLTK)\n",
        "\n",
        "The data will be treated with the bag of words (BoW)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUPQlwN68LVW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import nltk"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThKeSp2J8cEn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsq9PBEI8vv3",
        "outputId": "6f43f2f9-1429-4095-db57-905383dc80e5"
      },
      "source": [
        "from defs import get_tokens\n",
        "from defs import get_lemmas\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sms = pd.read_csv('datasets/sms_spam_no_header.csv', sep=',', names = [\"type\", \"text\"])\n",
        "text_train, text_test, type_train, type_test = train_test_split(sms['text'], sms['type'], test_size = 0.3)\n",
        "#bow stands for \"Bag of Words\"\n",
        "\n",
        "bow = CountVectorizer(analyzer=get_lemmas).fit(text_train)\n",
        "sms_bow = bow.transform(text_train)\n",
        "tfidf = TfidfTransformer().fit(sms_bow)\n",
        "sms_tfidf = tfidf.transform(sms_bow)\n",
        "spam_detector = MultinomialNB().fit(sms_tfidf, type_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34uz2G4j-UYq",
        "outputId": "e1aa3d47-cf05-46bb-f8b0-dd1f483b0feb"
      },
      "source": [
        "msg = sms['text'][25]\n",
        "msg_bow = bow.transform([msg])\n",
        "msg_tfidf = tfidf.transform(msg_bow)\n",
        "\n",
        "print('predicted: ', spam_detector.predict(msg_tfidf)[0])\n",
        "print('expected: ', sms.type[25])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted:  ham\n",
            "expected:  ham\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7UczIUj-2f7",
        "outputId": "5b1902b4-c5aa-474e-9749-364865f29d9c"
      },
      "source": [
        "msg = sms['text'][11]\n",
        "msg_bow = bow.transform([msg])\n",
        "msg_tfidf = tfidf.transform(msg_bow)\n",
        "\n",
        "print('predicted: ', spam_detector.predict(msg_tfidf)[0])\n",
        "print('expected: ', sms.type[11])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted:  spam\n",
            "expected:  spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqiyRfvi--QE",
        "outputId": "965c1cfe-dc3c-4f56-f52f-968a5b2d3320"
      },
      "source": [
        "predictions = spam_detector.predict(sms_tfidf)\n",
        "print('accuracy', accuracy_score(sms['type'][:len(predictions)], predictions))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7918482440399898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfwY0Z-y_WOv",
        "outputId": "1d632c10-32d7-4481-c475-9ddcdb9da2f7"
      },
      "source": [
        "print(classification_report(sms['type'][:len(predictions)], predictions))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.87      0.90      0.88      3382\n",
            "        spam       0.13      0.10      0.11       519\n",
            "\n",
            "    accuracy                           0.79      3901\n",
            "   macro avg       0.50      0.50      0.50      3901\n",
            "weighted avg       0.77      0.79      0.78      3901\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}